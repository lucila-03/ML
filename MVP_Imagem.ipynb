{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucila-03/ML/blob/main/MVP_Imagem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1PEQEdZ9zm4"
      },
      "source": [
        "## 1. Definição do Problema *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nQYT7kVTXkU"
      },
      "source": [
        "### Classificador de imagens binária - Pinguim e Tartaruga"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM86p08BWjzL"
      },
      "outputs": [],
      "source": [
        "# Importação dos pacotes\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "import gdown\n",
        "import os\n",
        "import zipfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import sklearn.metrics as skm\n",
        "import itertools\n",
        "import urllib.request\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from keras.optimizers import Adam\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from datetime import datetime\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import sklearn.metrics as skm\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ybokALgXsIB"
      },
      "source": [
        "#### Utilidades\n",
        "\n",
        "`plot_confusion_matrix` é uma função python que imprime uma matriz de confusão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjP-l-e1X-hz"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    Esta função imprime e plota a matriz de confusão.\n",
        "    A normalização pode ser aplicada definindo `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Matriz de confusão normalizada\")\n",
        "    else:\n",
        "        print('Matriz de confusão sem normalização')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Label real')\n",
        "    plt.xlabel('Label predito')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRC-zf-NYQHh"
      },
      "source": [
        "### 2. Carga de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae6j7TiUYtfl"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/lucila-03/ML\n",
        "\n",
        "train_dir = 'ML/train'\n",
        "val_dir = 'ML/test'\n",
        "test_dir = 'ML/valid'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfqLkI0-WJAs"
      },
      "outputs": [],
      "source": [
        "batch_size = 332\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "num_classes = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Cn6SD6NSax9"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Criação dos data generators\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    target_size=(img_height, img_width),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='binary') #training set\n",
        "\n",
        "val_generator = test_datagen.flow_from_directory(val_dir,\n",
        "                                                target_size=(img_height, img_width),\n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                target_size=(img_height, img_width),\n",
        "                                                batch_size=batch_size,\n",
        "                                                class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ksZuJxeJXLH"
      },
      "source": [
        "Após alguns testes, foi identificado que fazer um mix do relu com o elu indo até 512, demora um pouco mais, mas é o que apresenta melhores resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdEBpKz2P_LX"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='elu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='elu'),  # Nova camada Dense com 512 unidades e ELU\n",
        "    tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scD8i2-kp5sE"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy' , metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBouXitUJmCy"
      },
      "source": [
        "Após várias tentativas, 60 epochs é o número mais próximo do ideal, talvez com o aumento das imagens de treino, a acurácia seja melhor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nf4w1HUHp_Kk"
      },
      "outputs": [],
      "source": [
        "epochs = 60\n",
        "history = model.fit(train_generator, epochs=epochs, validation_data=val_generator)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WgwiIzlAR5G"
      },
      "source": [
        "Com a acurácia apresentada, talvez seja interessante saber a quantidade de imagens em cada pasta se houve diferença significativa."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista para armazenar o total de imagens em cada diretório\n",
        "total_imagens_por_figura = []\n",
        "\n",
        "# Lista para armazenar os nomes dos diretórios\n",
        "nome_pastas = []\n",
        "\n",
        "# Percorrendo os diretórios\n",
        "for diretorio in os.listdir(train_dir):\n",
        "    # Verificando se é um diretório\n",
        "    if os.path.isdir(os.path.join(train_dir, diretorio)):\n",
        "        # Contando o total de imagens no diretório\n",
        "        total_imagens = len(os.listdir(os.path.join(train_dir, diretorio)))\n",
        "        total_imagens_por_figura.append(total_imagens)\n",
        "        nome_pastas.append(diretorio)\n",
        "\n",
        "# Criando o gráfico de barras\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(nome_pastas, total_imagens_por_figura)\n",
        "plt.xlabel('Diretórios')\n",
        "plt.ylabel('Total de Imagens')\n",
        "plt.title('Total de Imagens em Cada Diretório')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Adicionando os valores acima das barras\n",
        "for i, v in enumerate(total_imagens_por_figura):\n",
        "    plt.text(i, v, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ifmmH9amNC-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9r7fxFPZaio"
      },
      "source": [
        "Tem mais pinguim que tartaruga, a quantidde de imagens disponibilzadas no diretório é superior, mas a diferença na quantidade de imagens de pinguim e tartaruga não é significativa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxLGBm2ft7y2"
      },
      "outputs": [],
      "source": [
        "\n",
        "count_images = 0\n",
        "class_names = ['Tartaruga', 'Pinguim']\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "# Percorrendo a pasta onde estão salvas as imagens de teste\n",
        "for subdir, dirs, files in os.walk(test_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.png') or file.endswith('.jpg'):\n",
        "            count_images += 1\n",
        "            split_path = os.path.join(subdir, file).split('/')\n",
        "            label = split_path[2]\n",
        "            y_true.append(label)\n",
        "\n",
        "            img_path = os.path.join(subdir, file)\n",
        "            img = Image.open(img_path)\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')  # Para não mostrar os eixos\n",
        "            plt.show()\n",
        "\n",
        "            img = image.load_img(img_path, target_size=(img_height, img_width))\n",
        "            x = image.img_to_array(img)\n",
        "            x = np.expand_dims(x, axis=0)\n",
        "            x = x.astype('float32') / 255.0\n",
        "\n",
        "            # Previsão\n",
        "            prediction = model.predict(x)\n",
        "\n",
        "            # Printando as saídas do modelo\n",
        "            predicted_class = np.argmax(prediction[0])\n",
        "            probability = prediction[0][predicted_class]\n",
        "            y_pred.append(class_names[predicted_class])\n",
        "            print(\"Label:\", label)\n",
        "            print(\"Previsão:\", class_names[predicted_class])\n",
        "            print(\"Probabilidade:\", probability)\n",
        "            print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "É importante fazer uma análise da acurácia para identificar o desempenho do modelo"
      ],
      "metadata": {
        "id": "d32TVVh2j0oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lista todos os dados contidos no histórico de treinamento\n",
        "\n",
        "# plota a acurácia a partir do histórico\n",
        "plt.plot(history.history['accuracy'])  # acurácia do treinamento\n",
        "plt.plot(history.history['val_accuracy']) # acurácia na validação (teste)\n",
        "plt.title('Acurácia do Modelo')\n",
        "plt.ylabel('acurácia')\n",
        "plt.xlabel('época')\n",
        "plt.legend(['treino', 'teste'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# plota o erro/perda a partir do histórico\n",
        "plt.plot(history.history['loss']) # perda/erro durante o treinamento\n",
        "plt.plot(history.history['val_loss']) #perda/erro durante a validação (teste)\n",
        "plt.title('Perda do modelo')\n",
        "plt.ylabel('perda')\n",
        "plt.xlabel('época')\n",
        "plt.legend(['treino', 'teste'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8h-qmjdMeSyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O treino apresentou um desempenho melhor que o teste ao falarmos de acurácia, mas a perda ainda é significativa, a matriz de confusão deve dar uma visão melhor do desempenho."
      ],
      "metadata": {
        "id": "O_1WGTBwei_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazer as previsões no subconjunto de imagens de teste\n",
        "y_pred = model.predict(test_generator)\n",
        "#y_pred = np.argmax(y_pred, axis=1)  # Converte as previsões para as classes preditas\n",
        "\n",
        "# Aplicar a regra de atribuição de 0 ou 1\n",
        "for i in range(len(y_pred)):\n",
        "    if y_pred[i] < 0.5:\n",
        "        y_pred[i] = 0\n",
        "    else:\n",
        "        y_pred[i] = 1\n",
        "# Obter os rótulos verdadeiros do subconjunto de imagens de teste\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Calcular a matriz de confusão\n",
        "confusion = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Criar um heatmap da matriz de confusão\n",
        "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "\n",
        "# Definir rótulos dos eixos\n",
        "plt.xlabel('Classe Predita')\n",
        "plt.ylabel('Classe Verdadeira')\n",
        "\n",
        "# Definir título do gráfico\n",
        "plt.title('Matriz de Confusão')\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zKBYXnc2dwW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Conclusão"
      ],
      "metadata": {
        "id": "751OTvSEjnS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo apresentado não é o ideal, ainda apresenta lacunas de aprendizado, principalmente na validação, um banco de imagens muito maior seria mais apropriado, porque a perda é significativa. Portanto, não vale a pena salvar um modelo que tem uma perda tão significativa."
      ],
      "metadata": {
        "id": "oB8_UuEKelhq"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXcYf4UAfEzTQU1tskfiBN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}